# distributed-training

This repository provides an example of distributed training using PyTorch on the ImageNet dataset. It demonstrates how to efficiently scale deep learning training across multiple GPUs and nodes using PyTorch's torch.distributed module. The code is built on the official PyTorch ImageNet example and includes enhancements for better scalability, fault tolerance, and performance monitoring. 
